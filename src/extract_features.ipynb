{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data from JSON file\n",
    "- Create empty dataframe\n",
    "- Loop on every question/answer pair in the file\n",
    "- Add a row to the dataframe for each pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "question_df = pd.DataFrame(columns=[\"Question\",\"Context\",\"Text answer\",\"Answer start\"])\n",
    "\n",
    "FILE_PATH = \"../data/\"\n",
    "\n",
    "with open(FILE_PATH+'train.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for book in data['data']:\n",
    "        for paragraph in book['paragraphs']:\n",
    "            for question in paragraph['qas']:\n",
    "                row = pd.Series(data={'Question' : question['question'],\n",
    "                                      'Context' : paragraph['context'],\n",
    "                                      'Text answer': question['answers'][0]['answer_start'],\n",
    "                                      'Answer start': question['answers'][0]['text']})\n",
    "                question_df = question_df.append(row,ignore_index=True)\n",
    "                \n",
    "with open(FILE_PATH+'valid.json') as json_file:\n",
    "    data = json.load(json_file)\n",
    "    for book in data['data']:\n",
    "        for paragraph in book['paragraphs']:\n",
    "            for question in paragraph['qas']:\n",
    "                row = pd.Series(data={'Question' : question['question'],\n",
    "                                      'Context' : paragraph['context'],\n",
    "                                      'Text answer': question['answers'][0]['answer_start'],\n",
    "                                      'Answer start': question['answers'][0]['text']})\n",
    "                question_df = question_df.append(row,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Context</th>\n",
       "      <th>Text answer</th>\n",
       "      <th>Answer start</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Quel astronome a émit l'idée en premier d'une ...</td>\n",
       "      <td>L'idée selon laquelle une planète inconnue pou...</td>\n",
       "      <td>136</td>\n",
       "      <td>Johann Elert Bode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quel astronome découvrit Uranus ?</td>\n",
       "      <td>L'idée selon laquelle une planète inconnue pou...</td>\n",
       "      <td>404</td>\n",
       "      <td>William Herschel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Quelles furent les découvertes finales des vin...</td>\n",
       "      <td>L'idée selon laquelle une planète inconnue pou...</td>\n",
       "      <td>733</td>\n",
       "      <td>plusieurs autres astéroïdes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Quelles furent les découvertes finales des vin...</td>\n",
       "      <td>L'idée selon laquelle une planète inconnue pou...</td>\n",
       "      <td>733</td>\n",
       "      <td>plusieurs autres astéroïdes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Combien de fois Piazzi est-il parvenu à observ...</td>\n",
       "      <td>Piazzi observa Cérès 24 fois, la dernière fois...</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23914</th>\n",
       "      <td>A quel risque la zone où se situe la chapelle ...</td>\n",
       "      <td>La chapelle se trouvant dans une zone inondabl...</td>\n",
       "      <td>33</td>\n",
       "      <td>zone inondable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23915</th>\n",
       "      <td>Comment était disposés les murets par rapport ...</td>\n",
       "      <td>La chapelle se trouvant dans une zone inondabl...</td>\n",
       "      <td>210</td>\n",
       "      <td>perpendiculaires au courant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23916</th>\n",
       "      <td>En quelle matière sont les dalles du canal ?</td>\n",
       "      <td>La chapelle se trouvant dans une zone inondabl...</td>\n",
       "      <td>322</td>\n",
       "      <td>schiste</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23917</th>\n",
       "      <td>Par quoi le canal est-il doublé ?</td>\n",
       "      <td>La chapelle se trouvant dans une zone inondabl...</td>\n",
       "      <td>479</td>\n",
       "      <td>par une digue</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23918</th>\n",
       "      <td>A quoi devaient servir les terrasses ?</td>\n",
       "      <td>La chapelle se trouvant dans une zone inondabl...</td>\n",
       "      <td>1060</td>\n",
       "      <td>terrains agricoles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23919 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Question  \\\n",
       "0      Quel astronome a émit l'idée en premier d'une ...   \n",
       "1                      Quel astronome découvrit Uranus ?   \n",
       "2      Quelles furent les découvertes finales des vin...   \n",
       "3      Quelles furent les découvertes finales des vin...   \n",
       "4      Combien de fois Piazzi est-il parvenu à observ...   \n",
       "...                                                  ...   \n",
       "23914  A quel risque la zone où se situe la chapelle ...   \n",
       "23915  Comment était disposés les murets par rapport ...   \n",
       "23916       En quelle matière sont les dalles du canal ?   \n",
       "23917                  Par quoi le canal est-il doublé ?   \n",
       "23918             A quoi devaient servir les terrasses ?   \n",
       "\n",
       "                                                 Context Text answer  \\\n",
       "0      L'idée selon laquelle une planète inconnue pou...         136   \n",
       "1      L'idée selon laquelle une planète inconnue pou...         404   \n",
       "2      L'idée selon laquelle une planète inconnue pou...         733   \n",
       "3      L'idée selon laquelle une planète inconnue pou...         733   \n",
       "4      Piazzi observa Cérès 24 fois, la dernière fois...          21   \n",
       "...                                                  ...         ...   \n",
       "23914  La chapelle se trouvant dans une zone inondabl...          33   \n",
       "23915  La chapelle se trouvant dans une zone inondabl...         210   \n",
       "23916  La chapelle se trouvant dans une zone inondabl...         322   \n",
       "23917  La chapelle se trouvant dans une zone inondabl...         479   \n",
       "23918  La chapelle se trouvant dans une zone inondabl...        1060   \n",
       "\n",
       "                      Answer start  \n",
       "0                Johann Elert Bode  \n",
       "1                 William Herschel  \n",
       "2      plusieurs autres astéroïdes  \n",
       "3      plusieurs autres astéroïdes  \n",
       "4                               24  \n",
       "...                            ...  \n",
       "23914               zone inondable  \n",
       "23915  perpendiculaires au courant  \n",
       "23916                      schiste  \n",
       "23917                par une digue  \n",
       "23918           terrains agricoles  \n",
       "\n",
       "[23919 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(question_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText \n",
    "- CBOW Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft_cbow = gensim.models.fasttext.FastText.load('../models/fast_cbow_300D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Skip-Gram Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ft_sg = gensim.models.fasttext.FastText.load('../models/fast_sg_300D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "- CBOW Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_cbow = gensim.models.Word2Vec.load('../models/w2v_cbow_300D')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Skip-Gram Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v_sg = gensim.models.Word2Vec.load('../models/w2v_sg_300D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.models import KeyedVectors\n",
    "import fasttext\n",
    "# Load vector directly from the file\n",
    "def load_model():\n",
    "    try:\n",
    "        # model = KeyedVectors.load_word2vec_format('../model/frWac_postag_no_phrase_1000_skip_cut100.bin', binary=True)\n",
    "        model = fasttext.load_model(\"../models/cc.fr.300.bin\")\n",
    "        print(\"model loaded\")\n",
    "        return model\n",
    "    except:\n",
    "        print(\"model not found\")\n",
    "        \n",
    "model = load_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute word vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_WordVector(word,p_model):\n",
    "    # Check if a word exists in the model\n",
    "    try:\n",
    "        vector = p_model[word]\n",
    "        return vector\n",
    "    except:\n",
    "        vector = model_ft_sg[word]\n",
    "        return vector    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute sentence vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_SentenceVector(sentence,p_model):\n",
    "    list_word_vectors = list()\n",
    "    for word in sentence:\n",
    "        word_vec = get_WordVector(word,p_model)\n",
    "        list_word_vectors.append(word_vec)\n",
    "    return sum(list_word_vectors)/len(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute cosine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "\n",
    "def get_cosine_distance(word_vec_1,word_vec_2):\n",
    "    return (1 - spatial.distance.cosine(word_vec_1, word_vec_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_cos_distance(word_vec_1,word_vec_2):\n",
    "    return cosine_similarity([word_vec_1], [word_vec_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt \n",
    "def euclidean_similarity(x,y):\n",
    "    edist = sqrt(sum(pow(a-b,2) for a, b in zip(x, y)))\n",
    "    edist = edist/10\n",
    "    return 1-edist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nsentence_vec_1 = get_SentenceVector([\"Je\",\"mange\",\"des\",\"nougats\",\"le\",\"matin\"])\\nsentence_vec_2 = get_SentenceVector([\"Je\",\"mange\",\"des\",\"pommes\"])\\nprint(get_cosine_distance(sentence_vec_1,sentence_vec_2))\\n\\nprint(\"---------\")\\nsentence_vec_1 = get_SentenceVector([\"Je\",\"dors\",\"la\",\"nuit\",\"et\",\"je\",\"vole\",\"le\",\"jour\"])\\nsentence_vec_2 = get_SentenceVector([\"Je\",\"mange\",\"des\",\"pommes\"])\\nprint(get_cosine_distance(sentence_vec_1,sentence_vec_2))\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "sentence_vec_1 = get_SentenceVector([\"Je\",\"mange\",\"des\",\"nougats\",\"le\",\"matin\"])\n",
    "sentence_vec_2 = get_SentenceVector([\"Je\",\"mange\",\"des\",\"pommes\"])\n",
    "print(get_cosine_distance(sentence_vec_1,sentence_vec_2))\n",
    "\n",
    "print(\"---------\")\n",
    "sentence_vec_1 = get_SentenceVector([\"Je\",\"dors\",\"la\",\"nuit\",\"et\",\"je\",\"vole\",\"le\",\"jour\"])\n",
    "sentence_vec_2 = get_SentenceVector([\"Je\",\"mange\",\"des\",\"pommes\"])\n",
    "print(get_cosine_distance(sentence_vec_1,sentence_vec_2))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "def rm_stop_words_nltk(sentence):\n",
    "    stopWords = set(stopwords.words('french'))\n",
    "    words = word_tokenize(sentence)\n",
    "    wordsFiltered = []\n",
    "    for w in words:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    return wordsFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "final_stopwords_list = list(fr_stop)\n",
    "\n",
    "def rm_stop_words_spacy(sentence):\n",
    "    final_stopwords_list = list(fr_stop)\n",
    "    words = word_tokenize(sentence)\n",
    "    wordsFiltered = []\n",
    "    for w in words:\n",
    "        if w not in final_stopwords_list:\n",
    "            wordsFiltered.append(w)\n",
    "    return wordsFiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dors', 'nuit', 'vole', 'jour', 'lit', 'fait', 'taille', 'mongolfiere']\n",
      "['dors', 'nuit', 'vole', 'jour', 'lit', 'taille', 'mongolfiere']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"je dors la nuit et je vole le jour sur un lit qui fait la taille de une mongolfiere\"\n",
    "print(rm_stop_words_nltk(sentence))\n",
    "print(rm_stop_words_spacy(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove all non-alphanumeric characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bonjour comment allez vous\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def remove_non_alphanum_chars(sentence):\n",
    "    return re.sub(r'([^\\s\\w]|_)+', '', sentence)\n",
    "\n",
    "import string\n",
    "def remove_non_alphanum_chars(s):\n",
    "    return s.translate(str.maketrans('', '', string.punctuation))\n",
    "print (remove_non_alphanum_chars(\"bonjour, comment allez vous?\"))\n",
    "\n",
    "\n",
    "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je mappelle Basile je suis pauvre Je mange des gateaux\n"
     ]
    }
   ],
   "source": [
    "print(remove_non_alphanum_chars(\"Je m'appelle Basile, je suis pauvre. Je mange des gateaux.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    # Add pre-processing steps\n",
    "    sentence = remove_non_alphanum_chars(sentence)\n",
    "    sentence = rm_stop_words_spacy(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get sentence answer number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_answer_num(sent_list,ans_start):\n",
    "    matched_indexes = []\n",
    "    #print(\"ans_start : \",ans_start)\n",
    "    i = -1\n",
    "    cpt = 0\n",
    "    while cpt < ans_start:\n",
    "        cpt = cpt + len(sent_list[i+1])\n",
    "        i += 1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_answer_num(sent_list,ans):\n",
    "    matched_indexes = []\n",
    "    #print(\"ans_start : \",ans_start)\n",
    "    ind = 0\n",
    "    for index,sent in enumerate(sent_list):\n",
    "        if ans in sent:\n",
    "            ind = index\n",
    "            break\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split text into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "list_len_context = list()\n",
    "feature_df = pd.DataFrame(columns=[\"cos_dist_0\",\n",
    "                                   \"cos_dist_1\",\n",
    "                                   \"cos_dist_2\",\n",
    "                                   \"cos_dist_3\",\n",
    "                                   \"cos_dist_4\",\n",
    "                                   \"cos_dist_5\",\n",
    "                                   \"cos_dist_6\",\n",
    "                                   \"cos_dist_7\",\n",
    "                                   \"cos_dist_8\",\n",
    "                                   \"cos_dist_9\",\n",
    "                                   \"cos_dist_10\",\n",
    "                                   \"True Index\",\"Pred Index\"])\n",
    "feature_df_2 = pd.DataFrame(columns=[\"Answer\",\"Question\"])\n",
    "\n",
    "true_cpt = 0\n",
    "false_cpt = 0\n",
    "\n",
    "\n",
    "for index, row in question_df.iterrows():\n",
    "    # print(\"--------\")\n",
    "    splitted_context = nltk.tokenize.sent_tokenize(row['Context'])   \n",
    "    index_answer = get_sentence_answer_num(splitted_context,row['Answer start'])\n",
    "    list_cos_distance = list()\n",
    "    list_sents = list()\n",
    "    question = row['Question']\n",
    "    preprocessed_question = preprocess(question.lower())\n",
    "    # print(splitted_context)\n",
    "    # print(preprocessed_question)\n",
    "\n",
    "    question_vector = get_SentenceVector(preprocessed_question,model_ft_sg)\n",
    "    if len(splitted_context) <= 10:\n",
    "        list_len_context.append(len(splitted_context))\n",
    "        for sentence in splitted_context:\n",
    "            preprocessed_sentence = preprocess(sentence.lower())\n",
    "            if len(preprocessed_sentence) != 0:\n",
    "                sentence_vector = get_SentenceVector(preprocessed_sentence,model_ft_sg)\n",
    "                list_sents.append(sentence_vector)\n",
    "                cos_distance = get_cosine_distance(sentence_vector,question_vector)\n",
    "                list_cos_distance.append(cos_distance)\n",
    "            else:\n",
    "                list_cos_distance.append(0)\n",
    "                list_sents.append(0)\n",
    "\n",
    "        # print(row['Context'])\n",
    "        # print(row['Question'])\n",
    "        # print(row['Answer start'])\n",
    "\n",
    "        # print(list_cos_distance)\n",
    "        index = list_cos_distance.index(max(list_cos_distance))\n",
    "        # print(\"Predicted index : \",index)\n",
    "        # print(\"index_answer : \",index_answer)\n",
    "        # print(\"Guessed sentence : \",list_sents[index])\n",
    "        # print(\"Real sentence : \",list_sents[index_answer])\n",
    "\n",
    "        # print(\"Guessed cos : \",list_cos_distance[index])\n",
    "        # print(\"Real cos : \",list_cos_distance[index_answer])\n",
    "\n",
    "        if index == index_answer:\n",
    "            true_cpt+=1\n",
    "        else :\n",
    "            false_cpt+=1\n",
    "        \n",
    "        while len(list_cos_distance) <= 10:\n",
    "            list_cos_distance.append(0)\n",
    "            \n",
    "        row = pd.Series(data={'cos_dist_0' : list_cos_distance[0],\n",
    "                              'cos_dist_1' : list_cos_distance[1],\n",
    "                              'cos_dist_2' : list_cos_distance[2],\n",
    "                              'cos_dist_3' : list_cos_distance[3],\n",
    "                              'cos_dist_4' : list_cos_distance[4],\n",
    "                              'cos_dist_5' : list_cos_distance[5],\n",
    "                              'cos_dist_6' : list_cos_distance[6],\n",
    "                              'cos_dist_7' : list_cos_distance[7],\n",
    "                              'cos_dist_8' : list_cos_distance[8],\n",
    "                              'cos_dist_9' : list_cos_distance[9],\n",
    "                              'cos_dist_10' : list_cos_distance[10],\n",
    "                              'True Index': index_answer, \n",
    "                              'Pred Index': index})\n",
    "        feature_df = feature_df.append(row,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.747976928374656\n",
      "10\n",
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.hist(list_len_context, density=True, bins=max(list_len_context))  # `density=False` would make counts\n",
    "plt.ylabel('Probability')\n",
    "plt.xlabel('Data');\n",
    "# Python program to get average of a list \n",
    "def Average(lst): \n",
    "    return sum(lst) / len(lst) \n",
    "\n",
    "print(Average(list_len_context))\n",
    "print(max(list_len_context))\n",
    "print(min(list_len_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15893\n",
      "7339\n"
     ]
    }
   ],
   "source": [
    "print(true_cpt)\n",
    "print(false_cpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.684099517906336\n"
     ]
    }
   ],
   "source": [
    "def accuracy(target, predicted):\n",
    "    \n",
    "    acc = (target==predicted).sum()/len(target)\n",
    "    \n",
    "    return acc\n",
    "\n",
    "print(accuracy(feature_df[\"True Index\"], feature_df[\"Pred Index\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the dataframe to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.to_csv('../data/train.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
